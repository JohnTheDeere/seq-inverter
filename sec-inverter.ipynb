{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcb56f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinporkholm/opt/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b85ae",
   "metadata": {},
   "source": [
    "### What does this notebook do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9672114",
   "metadata": {},
   "source": [
    "A simple transformer based encoder-decoder architecture is trained to flip the input of binary values e.g. [0, 1, 1, 0, 0, 1] -> [1, 0, 0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e575da5",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8263a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "def sinusoids(length, channels, max_timescale=5000):\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "\n",
    "def create_mask(tgt):\n",
    "    mask = (torch.triu(torch.ones((tgt.shape[1], tgt.shape[1]), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, True).masked_fill(mask == 1, False)\n",
    "    return mask.type(torch.bool)\n",
    "\n",
    "\n",
    "# Helper class to track losses\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val   = 0\n",
    "        self.avg   = 0\n",
    "        self.sum   = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val    = val\n",
    "        self.sum   += val * n\n",
    "        self.count += n\n",
    "        self.avg    = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96eb53",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66204fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N          = number of training examples\n",
    "# seq_length = maximum sequence length\n",
    "# inputs     = N randomly generated training examples of binaries\n",
    "# outputs    = flipped version of inputs, what the model should learn to predict\n",
    "N          = 5000\n",
    "seq_length = 10\n",
    "inputs     = torch.randint(0,2,(N,seq_length))\n",
    "outputs    = torch.fliplr(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c394ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        # Build items in class instance\n",
    "        self.x = x.float()\n",
    "        self.y = y.float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690145f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = ToDataset(inputs, outputs), batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fcedf",
   "metadata": {},
   "source": [
    "### Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8498699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter number is : 2.114 million\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim = 256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "                \n",
    "        # Encoder embedding\n",
    "        self.encoder_embedding = nn.Linear(1,self.emb_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(d_model=self.emb_dim,\n",
    "                                       dim_feedforward=256,\n",
    "                                       nhead=4,\n",
    "                                       num_encoder_layers=2,\n",
    "                                       num_decoder_layers=2,\n",
    "                                       batch_first=True)\n",
    "        \n",
    "        \n",
    "        # Decoder embedding\n",
    "        self.decoder_embedding = nn.Linear(1,self.emb_dim)\n",
    "        \n",
    "        # Decoder projection\n",
    "        self.decoder_projection = nn.Linear(self.emb_dim, 1)\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.src_norm = nn.LayerNorm(self.emb_dim)\n",
    "        self.tgt_norm = nn.LayerNorm(self.emb_dim)\n",
    "                        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, tgt_mask: Tensor):\n",
    "        \n",
    "        # Source projection\n",
    "        src = torch.unsqueeze(src,-1)\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        src_emb = self.src_norm(src_emb)\n",
    "        src_emb = src_emb + sinusoids(src.shape[1], self.emb_dim, max_timescale=200)\n",
    "        \n",
    "        # Target projection\n",
    "        tgt = torch.unsqueeze(tgt, -1)\n",
    "        tgt_emb = self.decoder_embedding(tgt)\n",
    "        tgt_emb = self.tgt_norm(tgt_emb)\n",
    "        tgt_emb = tgt_emb + sinusoids(tgt.shape[1], self.emb_dim, max_timescale=200)\n",
    "            \n",
    "        # Transformer encoder-decoder\n",
    "        outs = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Decoder projection\n",
    "        return self.decoder_projection(outs)\n",
    "    \n",
    "    def encode(self, src: Tensor):\n",
    "        \n",
    "        # Source projection\n",
    "        src = torch.unsqueeze(src,-1)\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        src_emb = self.src_norm(src_emb)\n",
    "        src_emb = src_emb + sinusoids(src.shape[1], self.emb_dim, max_timescale=200)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        return self.transformer.encoder(src_emb)\n",
    "    \n",
    "    \n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        \n",
    "        # Target projection\n",
    "        tgt = torch.unsqueeze(tgt, -1)\n",
    "        tgt_emb = self.decoder_embedding(tgt)\n",
    "        tgt_emb = self.tgt_norm(tgt_emb)\n",
    "        tgt_emb = tgt_emb + sinusoids(tgt.shape[1], self.emb_dim, max_timescale=200)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        outs =  self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        \n",
    "        return self.decoder_projection(outs)\n",
    "    \n",
    "        \n",
    "# Initialize model\n",
    "model = Model(emb_dim = 256)\n",
    "\n",
    "# Number of parameters in model\n",
    "trainables = [p for p in model.parameters() if p.requires_grad]\n",
    "print('Total parameter number is : {:.3f} million'.format(sum(p.numel() for p in model.parameters()) / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c806300",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2b3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(train_loader, model):\n",
    "    \n",
    "    # Switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Binary cross entropy with logits loss & loss tracker\n",
    "    criterion, losses, optimizer = nn.BCEWithLogitsLoss(), AverageMeter(), Adam(model.parameters(),lr=0.0001)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "    \n",
    "        # Train in mini-batches\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "            # Get the inputs\n",
    "            src, tgt = data\n",
    "            tgt_input = torch.cat((torch.ones((tgt.shape[0],1)).fill_(0.0), tgt), -1)\n",
    "\n",
    "            tgt_input = tgt_input[:,:-1]\n",
    "            tgt_mask = create_mask(tgt_input)\n",
    "\n",
    "            # Forward + Backward\n",
    "            optimizer.zero_grad()      \n",
    "            outputs = model(src,tgt_input,tgt_mask)\n",
    "            loss = criterion(outputs, torch.unsqueeze(tgt,-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            losses.update(loss.data.cpu().numpy(), tgt.size(0))\n",
    "\n",
    "            # Print info\n",
    "            if batch_idx % 40 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch+1, batch_idx, len(train_loader), 100. * batch_idx / len(train_loader), loss=losses))\n",
    "        print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6482bef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/157 (0%)]\tLoss 0.6924 (0.6924)\t\n",
      "Train Epoch: 1 [40/157 (25%)]\tLoss 0.6473 (0.6761)\t\n",
      "Train Epoch: 1 [80/157 (51%)]\tLoss 0.5954 (0.6468)\t\n",
      "Train Epoch: 1 [120/157 (76%)]\tLoss 0.3340 (0.5807)\t\n",
      "*****\n",
      "Train Epoch: 2 [0/157 (0%)]\tLoss 0.2191 (0.5085)\t\n",
      "Train Epoch: 2 [40/157 (25%)]\tLoss 0.0802 (0.4341)\t\n",
      "Train Epoch: 2 [80/157 (51%)]\tLoss 0.0775 (0.3712)\t\n",
      "Train Epoch: 2 [120/157 (76%)]\tLoss 0.0434 (0.3227)\t\n",
      "*****\n",
      "Train Epoch: 3 [0/157 (0%)]\tLoss 0.0211 (0.2879)\t\n",
      "Train Epoch: 3 [40/157 (25%)]\tLoss 0.0051 (0.2569)\t\n",
      "Train Epoch: 3 [80/157 (51%)]\tLoss 0.0049 (0.2320)\t\n",
      "Train Epoch: 3 [120/157 (76%)]\tLoss 0.0292 (0.2116)\t\n",
      "*****\n",
      "Train Epoch: 4 [0/157 (0%)]\tLoss 0.0063 (0.1971)\t\n",
      "Train Epoch: 4 [40/157 (25%)]\tLoss 0.0035 (0.1822)\t\n",
      "Train Epoch: 4 [80/157 (51%)]\tLoss 0.0100 (0.1692)\t\n",
      "Train Epoch: 4 [120/157 (76%)]\tLoss 0.0021 (0.1583)\t\n",
      "*****\n",
      "Train Epoch: 5 [0/157 (0%)]\tLoss 0.0057 (0.1494)\t\n",
      "Train Epoch: 5 [40/157 (25%)]\tLoss 0.0011 (0.1406)\t\n",
      "Train Epoch: 5 [80/157 (51%)]\tLoss 0.0019 (0.1328)\t\n",
      "Train Epoch: 5 [120/157 (76%)]\tLoss 0.0007 (0.1258)\t\n",
      "*****\n",
      "Train Epoch: 6 [0/157 (0%)]\tLoss 0.0118 (0.1201)\t\n",
      "Train Epoch: 6 [40/157 (25%)]\tLoss 0.0005 (0.1144)\t\n",
      "Train Epoch: 6 [80/157 (51%)]\tLoss 0.0105 (0.1094)\t\n",
      "Train Epoch: 6 [120/157 (76%)]\tLoss 0.0019 (0.1047)\t\n",
      "*****\n",
      "Train Epoch: 7 [0/157 (0%)]\tLoss 0.0009 (0.1008)\t\n",
      "Train Epoch: 7 [40/157 (25%)]\tLoss 0.0005 (0.0968)\t\n",
      "Train Epoch: 7 [80/157 (51%)]\tLoss 0.0027 (0.0931)\t\n",
      "Train Epoch: 7 [120/157 (76%)]\tLoss 0.0007 (0.0899)\t\n",
      "*****\n",
      "Train Epoch: 8 [0/157 (0%)]\tLoss 0.0161 (0.0871)\t\n",
      "Train Epoch: 8 [40/157 (25%)]\tLoss 0.0010 (0.0843)\t\n",
      "Train Epoch: 8 [80/157 (51%)]\tLoss 0.0010 (0.0815)\t\n",
      "Train Epoch: 8 [120/157 (76%)]\tLoss 0.0005 (0.0789)\t\n",
      "*****\n",
      "Train Epoch: 9 [0/157 (0%)]\tLoss 0.0004 (0.0767)\t\n",
      "Train Epoch: 9 [40/157 (25%)]\tLoss 0.0003 (0.0743)\t\n",
      "Train Epoch: 9 [80/157 (51%)]\tLoss 0.0105 (0.0722)\t\n",
      "Train Epoch: 9 [120/157 (76%)]\tLoss 0.0003 (0.0701)\t\n",
      "*****\n",
      "Train Epoch: 10 [0/157 (0%)]\tLoss 0.0004 (0.0683)\t\n",
      "Train Epoch: 10 [40/157 (25%)]\tLoss 0.0008 (0.0665)\t\n",
      "Train Epoch: 10 [80/157 (51%)]\tLoss 0.0037 (0.0648)\t\n",
      "Train Epoch: 10 [120/157 (76%)]\tLoss 0.0018 (0.0633)\t\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainModel(train_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee2edd",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95dc4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input x, true is the flipped array\n",
    "x    = torch.randint(0,2,(1,10)).float()\n",
    "true = torch.fliplr(x).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c015ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc340735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the flipped array\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606959db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 225.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we will flip x using the transformer and check if we get the same result as \"true\"\n",
    "model.eval()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Get the memory\n",
    "memory = model.encode(x)\n",
    "\n",
    "# SOS (Start Of Sentence token) is 0\n",
    "ys = torch.ones(1, 1).fill_(0.0)\n",
    "\n",
    "# Roll out one sample at a time\n",
    "for i in tqdm(range(seq_length)):\n",
    "    tgt_mask = create_mask(ys)\n",
    "    out = sigmoid(model.decode(ys, memory, tgt_mask))\n",
    "    out = torch.squeeze(out, -1).data\n",
    "    out = out[:,-1].view(1,1)\n",
    "    ys = torch.cat((ys, out), -1)\n",
    "    \n",
    "# A token with pr > 0.5 is 1, else 0\n",
    "predicted = np.where(ys.numpy() > 0.5, 1.0, 0.0).flatten()\n",
    "predicted = predicted[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d25cbbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the predicted array - inspect visually\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7964c980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the arrays are equal\n",
    "np.array_equal(predicted, true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "83cb243ce57aaa132b771e04921fcc1c145138d3a1b938321d0c799200c13355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
